{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow 2.0  \n",
    "- google公司\n",
    "- 深度学习框架 定位就是工具  核心 keras\n",
    "\n",
    "---\n",
    "\n",
    "TensorFlow 2.0 与 1.0 的主要区别体现在易用性、API 设计、执行模式以及生态系统整合等方面。以下是详细的对比：\n",
    "\n",
    "### **1. 执行模式：Eager Execution 默认启用**\n",
    "- **TensorFlow 1.x**  \n",
    "  - 基于**静态计算图**，需先定义计算图，再通过 `tf.Session` 执行。\n",
    "  - 代码冗余，调试困难（如使用 `tf.Print` 或 `sess.run()` 查看中间结果）。\n",
    "\n",
    "- **TensorFlow 2.0**  \n",
    "  - 默认启用 **Eager Execution**（即时执行），代码逐行运行，如同普通 Python。\n",
    "  - 动态图更易调试，支持直接打印变量值。\n",
    "  - 通过 `@tf.function` 将 Python 函数转换为静态图，兼顾性能与灵活性（基于 AutoGraph）。\n",
    "\n",
    "\n",
    "\n",
    "### **2. API 简化与 Keras 整合**\n",
    "- **TensorFlow 1.x**  \n",
    "  - API 分散且冗余：存在 `tf.layers`、`tf.contrib`、`slim` 等多个模块。\n",
    "  - `tf.contrib` 包含实验性功能，但稳定性差，2.0 中已移除。\n",
    "\n",
    "- **TensorFlow 2.0**  \n",
    "  - **Keras 成为官方高阶 API**：模型构建、训练、评估更简洁。\n",
    "  - 清理冗余 API，核心功能整合到 `tf.keras`、`tf.data` 等模块。\n",
    "  - 示例对比：\n",
    "    ```python\n",
    "    # TF 1.x（需手动管理图与会话）\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    y = tf.layers.dense(x, 10)\n",
    "    # TF 2.0（Keras 风格）\n",
    "    model = tf.keras.Sequential([tf.keras.layers.Dense(10, input_shape=(784,))])\n",
    "    ```\n",
    "\n",
    "\n",
    "\n",
    "### **3. 移除冗余机制**\n",
    "- **TensorFlow 1.x**  \n",
    "  - 需显式使用 `tf.Session`、`tf.placeholder`、`tf.global_variables_initializer()`。\n",
    "  - 变量初始化依赖 `sess.run(tf.global_variables_initializer())`。\n",
    "\n",
    "- **TensorFlow 2.0**  \n",
    "  - **移除 `tf.Session` 和 `tf.placeholder`**：无需手动管理会话或占位符。\n",
    "  - 变量即时初始化，代码更简洁。\n",
    "\n",
    "\n",
    "\n",
    "### **4. 训练流程优化**\n",
    "- **TensorFlow 1.x**  \n",
    "  - 手动定义优化器、梯度计算（如 `tf.gradients`）和更新操作。\n",
    "  - 自定义训练循环复杂。\n",
    "\n",
    "- **TensorFlow 2.0**  \n",
    "  - **推荐使用 `model.fit()`**：内置训练流程，适合常见场景。\n",
    "  - **灵活自定义训练**：结合 `GradientTape` 和优化器：\n",
    "    ```python\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(x)\n",
    "        loss = loss_fn(y, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    ```\n",
    "\n",
    "\n",
    "\n",
    "### **5. 数据输入与预处理**\n",
    "- **TensorFlow 1.x**  \n",
    "  - 使用 `tf.data` 或 `feed_dict` 输入数据，但集成不够紧密。\n",
    "\n",
    "- **TensorFlow 2.0**  \n",
    "  - **强化 `tf.data`**：高效数据管道，支持并行预处理。\n",
    "  - 无缝对接 Keras 模型：\n",
    "    ```python\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "    model.fit(dataset, epochs=5)\n",
    "    ```\n",
    "\n",
    "\n",
    "\n",
    "### **6. 模型保存与部署**\n",
    "- **TensorFlow 1.x**  \n",
    "  - 使用 `tf.saved_model` 或 `checkpoint` 保存模型，格式复杂。\n",
    "\n",
    "- **TensorFlow 2.0**  \n",
    "  - **统一使用 `SavedModel` 格式**：跨平台部署（TensorFlow Lite、TensorFlow.js、TensorFlow Serving）。\n",
    "  - 简化保存与加载：\n",
    "    ```python\n",
    "    model.save('model_path')  # 保存\n",
    "    loaded_model = tf.keras.models.load_model('model_path')  # 加载\n",
    "    ```\n",
    "\n",
    "\n",
    "\n",
    "### **7. 兼容性与迁移工具**\n",
    "- **TensorFlow 1.x 兼容性**  \n",
    "  - 2.0 提供 `tf.compat.v1` 模块支持旧版 API，但鼓励迁移。\n",
    "  - 使用脚本 `tf_upgrade_v2` 自动转换 1.x 代码。\n",
    "\n",
    "\n",
    "\n",
    "### **8. 性能优化**\n",
    "- **TensorFlow 2.0**  \n",
    "  - Eager Execution 下，通过 `@tf.function` 将关键代码转为静态图，接近 1.x 性能。\n",
    "  - 优化运行时和硬件支持（如 GPU/TPU 加速）。\n",
    "\n",
    "\n",
    "\n",
    "### **总结：TensorFlow 2.0 的优势**\n",
    "- **更易用**：Eager Execution、Keras API、简化代码。\n",
    "- **更灵活**：支持动态图与静态图混合编程。\n",
    "- **更统一**：API 清理与模块整合。\n",
    "- **更高效**：`tf.data` 和 `tf.function` 提升性能。\n",
    "- **更好的生态**：强化部署工具链（TFLite、TF.js 等）。\n",
    "\n",
    "\n",
    "\n",
    "### **迁移建议**\n",
    "- 新项目直接使用 TensorFlow 2.0。\n",
    "- 旧项目逐步替换冗余 API（如 `tf.Session`、`tf.placeholder`），利用 `tf.function` 优化性能。\n",
    "- 参考官方迁移指南：[TensorFlow 2.0 Migration Guide](https://www.tensorflow.org/guide/migrate)。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.19.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[ 5,  8],\n",
       "       [ 8, 13]], dtype=int32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor 张量\n",
    "x1 = [[1,2] , [2,3]]\n",
    "m = tf.matmul(x1,x1)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[1, 9],\n",
       "       [3, 6]], dtype=int32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = tf.constant([[1,9] , [3,6]])\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[ 2, 10],\n",
       "       [ 4,  7]], dtype=int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3 = tf.add(x2,1)  # 对每个元素做加法\n",
    "x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2, 10],\n",
       "       [ 4,  7]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以和numpy转化\n",
    "x3.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[ 2., 10.],\n",
       "       [ 4.,  7.]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.cast(x3 , tf.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[2, 4],\n",
       "       [4, 6]], dtype=int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x4 = np.ones([2,2])\n",
    "print(x4)\n",
    "x5 = tf.multiply(x1,2)\n",
    "x5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **常用基础操作分类**\n",
    "#### **(1) 逐元素运算**\n",
    "- `tf.add(x, y)` 或 `x + y`：逐元素加法。\n",
    "- `tf.subtract(x, y)` 或 `x - y`：逐元素减法。\n",
    "- `tf.divide(x, y)` 或 `x / y`：逐元素除法。\n",
    "- `tf.math.log(x)`：自然对数。\n",
    "- `tf.square(x)`：平方。\n",
    "\n",
    "#### **(2) 矩阵运算**\n",
    "- `tf.tf.multiply(a, b)` ：矩阵逐元素相乘。\n",
    "- `tf.matmul(a, b)` 或 `a @ b`：矩阵乘法。\n",
    "- `tf.linalg.inv(a)`：矩阵求逆（需可逆方阵）。\n",
    "- `tf.transpose(a)`：矩阵转置。\n",
    "\n",
    "#### **(3) 归约操作**\n",
    "- `tf.reduce_sum(x, axis)`：沿指定轴求和。\n",
    "- `tf.reduce_mean(x, axis)`：沿指定轴求均值。\n",
    "- `tf.reduce_max(x, axis)`：沿指定轴求最大值。\n",
    "- `tf.reduce_min(x, axis)`：沿指定轴求最小值。\n",
    "\n",
    "#### **(4) 激活函数**\n",
    "- `tf.nn.relu(x)`：ReLU 激活。\n",
    "- `tf.nn.sigmoid(x)`：Sigmoid 激活。\n",
    "- `tf.nn.softmax(x, axis)`：Softmax 归一化。\n",
    "\n",
    "#### **(5) 形状操作**\n",
    "- `tf.reshape(x, new_shape)`：调整张量形状。\n",
    "  ```python\n",
    "  a = tf.ones([3, 4])\n",
    "  b = tf.reshape(a, [2, 6])  # 形状变为 (2, 6)\n",
    "  ```\n",
    "- `tf.concat([a, b], axis)`：沿指定轴拼接张量。\n",
    "- `tf.split(x, num_split, axis)`：分割张量。\n",
    "\n",
    "#### **(6) 张量创建**\n",
    "- `tf.constant(value)`：创建常量张量。\n",
    "- `tf.zeros(shape)` 和 `tf.ones(shape)`：全零/全一张量。\n",
    "- `tf.random.normal(shape)`：正态分布随机张量。\n",
    "- `tf.Variable(initial_value)`：创建可训练变量。\n",
    "\n",
    "#### **(7) 梯度计算**\n",
    "- 使用 `tf.GradientTape` 自动求导：\n",
    "  ```python\n",
    "  x = tf.Variable(3.0)\n",
    "  with tf.GradientTape() as tape:\n",
    "      y = x ** 2\n",
    "  dy_dx = tape.gradient(y, x)  # 输出 6.0\n",
    "  ```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras.layers\n",
    "\n",
    "### **1. 基础层**\n",
    "| 层 | 说明 | 代码示例 |\n",
    "|------|------|------|\n",
    "| `Dense` | 全连接层，常用于 MLP（多层感知机） | `layers.Dense(64, activation='relu')` |\n",
    "| `Activation` | 独立激活函数层 | `layers.Activation('relu')` |\n",
    "| `Flatten` | 把多维数据展平成一维 | `layers.Flatten()` |\n",
    "| `Reshape` | 调整输入形状 | `layers.Reshape((28, 28, 1))` |\n",
    "| `Permute` | 交换张量维度顺序 | `layers.Permute((2, 1, 3))` |\n",
    "| `Lambda` | 允许自定义计算逻辑 | `layers.Lambda(lambda x: x ** 2)` |\n",
    "\n",
    "\n",
    "\n",
    "### **2. 卷积层（CNN 相关）**\n",
    "适用于图像处理任务。\n",
    "\n",
    "| 层 | 说明 | 代码示例 |\n",
    "|------|------|------|\n",
    "| `Conv1D` | 1D 卷积层（用于时间序列） | `layers.Conv1D(32, 3, activation='relu')` |\n",
    "| `Conv2D` | 2D 卷积层（用于图像） | `layers.Conv2D(64, (3,3), activation='relu')` |\n",
    "| `Conv3D` | 3D 卷积层（用于 3D 数据，如医学影像） | `layers.Conv3D(64, (3,3,3), activation='relu')` |\n",
    "| `SeparableConv2D` | 深度可分离卷积，提高计算效率 | `layers.SeparableConv2D(32, (3,3), activation='relu')` |\n",
    "| `DepthwiseConv2D` | 深度卷积（每个通道独立卷积） | `layers.DepthwiseConv2D((3,3))` |\n",
    "\n",
    "\n",
    "\n",
    "### **3. 池化层（Pooling）**\n",
    "用于降维，减少计算量，防止过拟合。\n",
    "\n",
    "| 层 | 说明 | 代码示例 |\n",
    "|------|------|------|\n",
    "| `MaxPooling1D` | 1D 最大池化 | `layers.MaxPooling1D(2)` |\n",
    "| `MaxPooling2D` | 2D 最大池化 | `layers.MaxPooling2D((2,2))` |\n",
    "| `MaxPooling3D` | 3D 最大池化 | `layers.MaxPooling3D((2,2,2))` |\n",
    "| `AveragePooling2D` | 2D 平均池化 | `layers.AveragePooling2D((2,2))` |\n",
    "| `GlobalMaxPooling2D` | 全局最大池化 | `layers.GlobalMaxPooling2D()` |\n",
    "| `GlobalAveragePooling2D` | 全局平均池化 | `layers.GlobalAveragePooling2D()` |\n",
    "\n",
    "\n",
    "\n",
    "### **4. 循环神经网络层（RNN）**\n",
    "适用于时间序列、自然语言处理等任务。\n",
    "\n",
    "| 层 | 说明 | 代码示例 |\n",
    "|------|------|------|\n",
    "| `SimpleRNN` | 简单 RNN | `layers.SimpleRNN(32, return_sequences=True)` |\n",
    "| `LSTM` | 长短时记忆网络 | `layers.LSTM(64, return_sequences=True)` |\n",
    "| `GRU` | 门控循环单元（比 LSTM 计算更快） | `layers.GRU(64, return_sequences=True)` |\n",
    "| `Bidirectional` | 双向 RNN | `layers.Bidirectional(layers.LSTM(64))` |\n",
    "\n",
    "\n",
    "\n",
    "### **5. 归一化和正则化层**\n",
    "用于防止过拟合，提高模型稳定性。\n",
    "\n",
    "| 层 | 说明 | 代码示例 |\n",
    "|------|------|------|\n",
    "| `BatchNormalization` | 批归一化，加速训练，提高稳定性 | `layers.BatchNormalization()` |\n",
    "| `Dropout` | 随机丢弃部分神经元，防止过拟合 | `layers.Dropout(0.5)` |\n",
    "| `SpatialDropout2D` | 作用于 2D 特征图的 Dropout | `layers.SpatialDropout2D(0.3)` |\n",
    "| `LayerNormalization` | 层归一化（适用于 Transformer） | `layers.LayerNormalization()` |\n",
    "| `GaussianNoise` | 添加高斯噪声，增强鲁棒性 | `layers.GaussianNoise(0.1)` |\n",
    "\n",
    "\n",
    "\n",
    "### **6. 嵌入层（Embedding）**\n",
    "适用于自然语言处理（NLP）。\n",
    "\n",
    "| 层 | 说明 | 代码示例 |\n",
    "|------|------|------|\n",
    "| `Embedding` | 把离散整数映射到连续向量 | `layers.Embedding(input_dim=10000, output_dim=128)` |\n",
    "\n",
    "\n",
    "\n",
    "## **7. 自注意力层（Transformer 相关）**\n",
    "适用于 NLP 和序列任务。\n",
    "\n",
    "| 层 | 说明 | 代码示例 |\n",
    "|------|------|------|\n",
    "| `MultiHeadAttention` | 多头自注意力机制 | `layers.MultiHeadAttention(num_heads=8, key_dim=64)` |\n",
    "\n",
    "\n",
    "\n",
    "### **8. 其他高级层**\n",
    "| 层 | 说明 | 代码示例 |\n",
    "|------|------|------|\n",
    "| `Concatenate` | 拼接多个张量 | `layers.Concatenate()([x1, x2])` |\n",
    "| `Add` | 按元素相加 | `layers.Add()([x1, x2])` |\n",
    "| `Subtract` | 按元素相减 | `layers.Subtract()([x1, x2])` |\n",
    "| `Multiply` | 按元素相乘 | `layers.Multiply()([x1, x2])` |\n",
    "| `Dot` | 点积计算 | `layers.Dot(axes=1)([x1, x2])` |\n",
    "\n",
    "\n",
    "\n",
    "### **总结**\n",
    "- **`Dense`**：全连接层，适用于 MLP。\n",
    "- **卷积层（`Conv2D`）+ 池化层（`MaxPooling2D`）**：用于 CNN 。\n",
    "- **循环层（`LSTM`，`GRU`）**：用于 RNN 任务。\n",
    "- **归一化/正则化（`BatchNormalization`，`Dropout`）**：用于优化模型训练。\n",
    "- **`Embedding`**：用于 NLP 任务。\n",
    "- **`MultiHeadAttention`**：用于 Transformer 模型。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Keras 的基本使用方法及常见参数**  \n",
    "\n",
    "#### **1. 导入 Keras**\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "```\n",
    "\n",
    "\n",
    "#### **2. 构建模型**\n",
    "Keras 提供 **Sequential（顺序模型）** 和 **Functional（函数式模型）** 两种构建方式。  \n",
    "\n",
    "##### **2.1 Sequential（顺序模型）**\n",
    "适用于简单的线性堆叠网络。  \n",
    "\n",
    "```python\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(10,)),  # 输入层（10个特征）\n",
    "    layers.Dense(32, activation='relu'),  # 隐藏层\n",
    "    layers.Dense(1, activation='sigmoid')  # 输出层（1个神经元，适用于二分类）\n",
    "])\n",
    "```\n",
    "##### **常见参数：**\n",
    "| 参数 | 说明 | 示例 |\n",
    "|------|------|------|\n",
    "| `units` | 神经元个数 | `Dense(64)` |\n",
    "| `activation` | 激活函数 | `'relu'`, `'sigmoid'`, `'softmax'` |\n",
    "| `input_shape` | 输入特征数 | `input_shape=(10,)` |\n",
    "| `kernel_initializer` | 权重初始化 | `'he_uniform'`, `'glorot_uniform'` |\n",
    "| `bias_initializer` | 偏置初始化 | `'zeros'`, `'ones'` |\n",
    "\n",
    "\n",
    "##### **2.2 Functional（函数式模型）**\n",
    "适用于复杂网络，如多输入、多输出或共享层模型。\n",
    "\n",
    "```python\n",
    "inputs = keras.Input(shape=(10,))\n",
    "x = layers.Dense(64, activation='relu')(inputs)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### **3. 编译模型**\n",
    "```python\n",
    "model.compile(\n",
    "    optimizer='adam',  # 优化器\n",
    "    loss='binary_crossentropy',  # 损失函数（二分类）\n",
    "    metrics=['accuracy']  # 评估指标\n",
    ")\n",
    "```\n",
    "\n",
    "##### **常见参数：**\n",
    "| 参数 | 说明 | 可选值 |\n",
    "|------|------|------|\n",
    "| `optimizer` | 优化器 | `'adam'`, `'sgd'`, `'rmsprop'`, `'adagrad'` |\n",
    "| `loss` | 损失函数 | `'mse'`, `'mae'`, `'categorical_crossentropy'`, `'binary_crossentropy'` |\n",
    "| `metrics` | 评估指标 | `'accuracy'`, `'mae'`, `'mse'`, `AUC()` |\n",
    "\n",
    "\n",
    "\n",
    "#### **4. 训练模型**\n",
    "```python\n",
    "history = model.fit(\n",
    "    X_train, y_train,  # 训练数据\n",
    "    epochs=10,  # 训练轮数\n",
    "    batch_size=32,  # 每批次样本数\n",
    "    validation_data=(X_val, y_val)  # 验证集\n",
    ")\n",
    "```\n",
    "##### **常见参数：**\n",
    "| 参数 | 说明 | 示例 |\n",
    "|------|------|------|\n",
    "| `x, y` | 训练数据 | `(X_train, y_train)` |\n",
    "| `epochs` | 训练轮数 | `10` |\n",
    "| `batch_size` | 每批次样本数 | `32` |\n",
    "| `validation_data` | 验证集 | `(X_val, y_val)` |\n",
    "| `verbose` | 输出日志级别 | `0`（静默），`1`（进度条），`2`（文本） |\n",
    "\n",
    "\n",
    "\n",
    "#### **5. 评估模型**\n",
    "```python\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### **6. 预测**\n",
    "```python\n",
    "predictions = model.predict(X_new)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### **7. 保存和加载模型**\n",
    "##### **7.1 保存模型**\n",
    "```python\n",
    "model.save('model.h5')  # 保存整个模型\n",
    "```\n",
    "##### **7.2 加载模型**\n",
    "```python\n",
    "model = keras.models.load_model('model.h5')\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.data 模块\n",
    "- 高效构建数据输入管道\n",
    "- 数据加载：支持从多种数据源（如内存数组、文件、生成器）创建数据集。\n",
    "- 数据转换：提供链式操作（如 map、filter、shuffle）处理数据。\n",
    "- 性能优化：通过预取（prefetch）、并行化（num_parallel_calls）和缓存（cache）加速数据流。\n",
    "- 批处理与序列处理：动态批处理、填充序列（如处理变长文本）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_data = np.arange(16)\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n",
      "tf.Tensor(10, shape=(), dtype=int64)\n",
      "tf.Tensor(11, shape=(), dtype=int64)\n",
      "tf.Tensor(12, shape=(), dtype=int64)\n",
      "tf.Tensor(13, shape=(), dtype=int64)\n",
      "tf.Tensor(14, shape=(), dtype=int64)\n",
      "tf.Tensor(15, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "data_set = tf.data.Dataset.from_tensor_slices(input_data)\n",
    "for data in data_set:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n",
      "tf.Tensor(10, shape=(), dtype=int64)\n",
      "tf.Tensor(11, shape=(), dtype=int64)\n",
      "tf.Tensor(12, shape=(), dtype=int64)\n",
      "tf.Tensor(13, shape=(), dtype=int64)\n",
      "tf.Tensor(14, shape=(), dtype=int64)\n",
      "tf.Tensor(15, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n",
      "tf.Tensor(10, shape=(), dtype=int64)\n",
      "tf.Tensor(11, shape=(), dtype=int64)\n",
      "tf.Tensor(12, shape=(), dtype=int64)\n",
      "tf.Tensor(13, shape=(), dtype=int64)\n",
      "tf.Tensor(14, shape=(), dtype=int64)\n",
      "tf.Tensor(15, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# repeat操作  重复数据集多次\n",
    "data_set = data_set.repeat(2)\n",
    "for data in data_set:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3], shape=(4,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7], shape=(4,), dtype=int64)\n",
      "tf.Tensor([ 8  9 10 11], shape=(4,), dtype=int64)\n",
      "tf.Tensor([12 13 14 15], shape=(4,), dtype=int64)\n",
      "tf.Tensor([0 1 2 3], shape=(4,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7], shape=(4,), dtype=int64)\n",
      "tf.Tensor([ 8  9 10 11], shape=(4,), dtype=int64)\n",
      "tf.Tensor([12 13 14 15], shape=(4,), dtype=int64)\n",
      "tf.Tensor([0 1 2 3], shape=(4,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7], shape=(4,), dtype=int64)\n",
      "tf.Tensor([ 8  9 10 11], shape=(4,), dtype=int64)\n",
      "tf.Tensor([12 13 14 15], shape=(4,), dtype=int64)\n",
      "tf.Tensor([0 1 2 3], shape=(4,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7], shape=(4,), dtype=int64)\n",
      "tf.Tensor([ 8  9 10 11], shape=(4,), dtype=int64)\n",
      "tf.Tensor([12 13 14 15], shape=(4,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# batch操作  合并数据为批次\n",
    "data_set = data_set.repeat(2).batch(4) # 4个为1批\n",
    "for data in data_set:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([12 13 14 15], shape=(4,), dtype=int64)\n",
      "tf.Tensor([ 8  9 10 11], shape=(4,), dtype=int64)\n",
      "tf.Tensor([0 1 2 3], shape=(4,), dtype=int64)\n",
      "tf.Tensor([0 1 2 3], shape=(4,), dtype=int64)\n",
      "tf.Tensor([12 13 14 15], shape=(4,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7], shape=(4,), dtype=int64)\n",
      "tf.Tensor([ 8  9 10 11], shape=(4,), dtype=int64)\n",
      "tf.Tensor([0 1 2 3], shape=(4,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7], shape=(4,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7], shape=(4,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7], shape=(4,), dtype=int64)\n",
      "tf.Tensor([12 13 14 15], shape=(4,), dtype=int64)\n",
      "tf.Tensor([12 13 14 15], shape=(4,), dtype=int64)\n",
      "tf.Tensor([0 1 2 3], shape=(4,), dtype=int64)\n",
      "tf.Tensor([ 8  9 10 11], shape=(4,), dtype=int64)\n",
      "tf.Tensor([ 8  9 10 11], shape=(4,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# shuffle操作  打乱数据顺序\n",
    "data_set = data_set.shuffle(buffer_size=1000)  # 缓冲区越大，随机性越强\n",
    "for data in data_set:\n",
    "    print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
